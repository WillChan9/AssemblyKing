# AssemblyKing
Be the KING of assembly.

## What does AssemblyKing do?

It's a program that can help trainee learn assembly parts from coach view, through AR prospective. 

## How to use it?

From the coach side:

Make sure your enviromenmt is in a clean manner and mechanical parts can be clearly seen through headset view. Then open the Cast function to cast your view to oculus website, which is the only way for a thrid party developer to get your camera view. 

Then run main.py, which can recording your current screen and synchronize to the website for trainees watching. To view the realtime screen recording in LAN, use the website pop up in the terminal, which might varied depends on your current networks, but use the same port, e.g.:

```
https://192.168.1.97:5001
```

From the trainee side:

Frist make sure to connect to the same LAN as coach, then open the browser and type in the website, you will see the coach view with object bounding box. Below the recording you can also see the instructions generated by VLM base on your current progress.

# Environment setup

Require Meta Segment Anything Model 2(SAM2), website: https://ai.meta.com/sam2/. Installation:
```
git clone https://github.com/facebookresearch/sam2.git && cd sam2 # clone to except current folder

pip install -e .
```

Download the Model config and checkpoint from: https://github.com/facebookresearch/sam2

Then install the rest of the packages.
```
pip3 install -r requirements.txt
```

# Introduction
## dataset_generation.py

This program takes object videos as input, and output the annotated images with labels that is ready for training. The key to generate accurate bounding box for the object is use Segment Anything Model2. It requires few manual clicks to set the object region. Then it will automatically spread through the entire video. The dataset contains a image folder with all the frames of a video, and a label folder with class name and bounding box location info. In this case, we have 6 objects and each object has 250-400 images for training, which requires at least 10 seconds of recording. During the recording, objects were flipped, move further and closer to enhance the robustness of dataset.

## model_training.py

The object detection model uses Yolov8. It takes 100 epochs to run on a CUDA GPU RTX 4060.

## main.py

Used to record the screen, apply object detection model and VLM analysis and share with other users.


